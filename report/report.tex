\documentclass[sigconf]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{Bib\TeX}}}

\setcopyright{none}
\acmYear{2026}
\acmDOI{}
\acmConference[Advanced NLP 2026]{Advanced NLP Exercise Report}{February 2026}{TU Berlin, Germany}
\acmISBN{}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{subcaption}

\begin{document}

\title{Multilingual ADE Detection and Classical ML vs.\ LLMs on Structured Data}

\author{Andrés Mandado Almajano}
\affiliation{%
  \institution{TU Berlin}
  \country{Germany}}
\email{mandadoalmajano@tu-berlin.de}

\renewcommand{\shortauthors}{Andrés Mandado Almajano}

\begin{abstract}
We present two NLP studies.
\textbf{Task~1} addresses multilingual detection of adverse drug events (ADEs) in social media using XLM-RoBERTa, comparing monolingual, multilingual, and machine-translation--based training across English, German, and French.
With 10 training epochs and early stopping, the multilingual EN+DE model outperforms all monolingual baselines on every language (EN F1\,=\,0.707, DE F1\,=\,0.484, FR zero-shot F1\,=\,0.321), reversing the initial finding that multilingual training hurts dominant-language performance.
Translation-trained French models collapse to all-negative predictions under natural class distribution, confirming MT artifacts as the fundamental bottleneck.
\textbf{Task~2} compares classical tree-based models, fine-tuned Transformers on textified data, and few-shot LLM prompting (FLAN-T5, Llama-3.2, Qwen2.5) on the Iris dataset.
The Random Forest and best Transformers both achieve 93.3\% macro-F1, while LLMs range from 16.7\% (FLAN-T5, single-class collapse) to 75.7\% (Qwen guided), demonstrating the fundamental mismatch between LLMs and small tabular tasks and the unreliability of LLM reasoning on numeric data.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010147.10010178.10010179</concept_id>
  <concept_desc>Computing methodologies~Natural language processing</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010257</concept_id>
  <concept_desc>Computing methodologies~Machine learning</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[300]{Computing methodologies~Machine learning}

\keywords{adverse drug events, multilingual NLP, cross-lingual transfer, machine translation, structured data, LLMs, textification}

\maketitle

%% ============================================================
\section{Introduction}

This report presents two complementary studies in applied NLP.
\textbf{Task~1} tackles SMM4H~2026 Shared Task~1~: binary detection of adverse drug events (ADEs) in multilingual social media posts (English tweets, German and French forum posts).
We study (i)~monolingual vs.\ multilingual training with XLM-RoBERTa~, (ii)~zero-shot cross-lingual transfer to unseen French, and (iii)~machine-translation--based training using MarianMT~ EN$\to$FR translations.
\textbf{Task~2} compares classical ML (Random Forest~), fine-tuned Transformer classifiers~, and few-shot LLM prompting~ on the Iris dataset~, investigating when modern NLP tools surpass or fall short of simple baselines on small structured data.

All experiments use the Hugging Face \texttt{transformers} library~ with fixed random seeds for reproducibility.

%% ============================================================
\section{Task~1: Multilingual ADE Detection}

\subsection{Data and Preprocessing}

The SMM4H~2026 Task~1 dataset contains social media posts labeled for ADE presence (binary).
We use English (EN, 17{,}128 train / 888 test), German (DE, 1{,}482 train / 634 test), and French (FR, 976 train / 418 test).
The provided development set is redefined as the \textbf{test set}; a new \textbf{validation set} (10\%) is created from the training data via stratified sampling by language$\times$label.
French data is held out entirely for zero-shot evaluation.

Table~\ref{tab:task1_data} summarizes key dataset statistics.
All languages exhibit severe class imbalance (${\sim}5$--$7$\% positive rate).
English data consists of short tweets ($\bar{w}\approx17$ words), while German and French are long forum posts ($\bar{w}\approx101$--$106$ words), creating a substantial domain mismatch.

\begin{table}[t]
  \caption{Task~1 dataset statistics (train split). $\bar{w}$: mean word count; $w_{95}$: 95th-percentile word count.}
  \label{tab:task1_data}
  \begin{tabular}{lrrrr}
    \toprule
    Lang & $n$ & Pos.\ rate & $\bar{w}$ & $w_{95}$ \\
    \midrule
    EN & 17{,}128 & 7.0\% & 16.5 & 27 \\
    DE & 1{,}482 & 5.6\% & 100.8 & 276 \\
    FR & 976 & 7.2\% & 105.8 & 313 \\
    \bottomrule
  \end{tabular}
\end{table}

Preprocessing is minimal and ADE-preserving: \texttt{@USER\_\_\_} patterns are collapsed to \texttt{@USER}, \texttt{HTTPURL\_\_\_} to \texttt{HTTPURL}, and the forum placeholder \texttt{<user>} is normalized.
No lowercasing is applied to preserve multilingual casing.

\paragraph{Machine translation.}
We translate 6{,}000 English training examples to French using the offline MarianMT model (\texttt{Helsinki-NLP/\allowbreak opus-mt-en-fr}), preserving the natural label distribution (${\sim}7$\% positive; 420~positive, 5{,}580~negative).
Translations are cached to disk; manual inspection reveals that hashtags, informal expressions, and social-media tokens are often transferred literally rather than adapted (see Appendix~\ref{app:samples}).

\paragraph{Data exploration.}
Figure~\ref{fig:label_dist} shows the label distribution across languages for the train and test splits.
All languages exhibit severe class imbalance (${\sim}5$--$7$\% positive rate), with EN dominating in absolute size.
Figure~\ref{fig:wordlen} plots word-length distributions: English tweets average ${\sim}17$~words, while German and French forum posts average ${\sim}101$--$106$ words, creating a substantial domain mismatch that affects both truncation behaviour and model generalisation.

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.48\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{label_dist_train.pdf}
    \caption{Train split.}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.48\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{label_dist_test.pdf}
    \caption{Test split.}
  \end{subfigure}
  \caption{Label distribution by language. All languages have ${\sim}5$--$7$\% positive rate; EN dominates in size.}
  \label{fig:label_dist}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=0.85\columnwidth]{word_length_dist.pdf}
  \caption{Word-length distribution (training set). Tweets (EN) are short; forum posts (DE/FR) are 5--6$\times$ longer, causing truncation at \texttt{max\_length=256}.}
  \label{fig:wordlen}
\end{figure}

\subsection{Modeling Setups}

All classifiers use \texttt{xlm-roberta-base} (278M parameters) with a sequence classification head, max length 256, and are trained for up to 10~epochs with early stopping (patience\,=\,3 on validation F1), learning rate $2{\times}10^{-5}$, batch size~16, and weight decay~0.01.
The same code path is used across four setups:
\begin{enumerate}
  \item \textbf{Mono-EN}: train on EN, evaluate on EN test.
  \item \textbf{Mono-DE}: train on DE, evaluate on DE test.
  \item \textbf{Multi EN+DE}: train on EN+DE jointly, evaluate on EN/DE/FR test.
  \item \textbf{Trans-FR}: train on EN$\to$FR translations, evaluate on FR test.
\end{enumerate}

\subsection{Results}

Table~\ref{tab:task1_results} reports binary F1, precision, recall, and accuracy for all setups.

\begin{table}[t]
  \caption{Task~1 evaluation results on the test set. Best F1 per language in \textbf{bold}. (zs)\,=\,zero-shot.}
  \label{tab:task1_results}
  \begin{tabular}{llrrrr}
    \toprule
    Setup & Lang & Prec. & Rec. & F1 & Acc. \\
    \midrule
    Mono-EN       & EN & .655 & .590 & .621 & .950 \\
    Multi EN+DE   & EN & .745 & .672 & \textbf{.707} & .962 \\
    \midrule
    Mono-DE       & DE & .000 & .000 & .000 & .945 \\
    Multi EN+DE   & DE & .393 & .629 & \textbf{.484} & .926 \\
    \midrule
    Mono-EN (zs)  & FR & .109 & .333 & .164 & .756 \\
    Mono-DE (zs)  & FR & .000 & .000 & .000 & .928 \\
    Multi EN+DE   & FR & .206 & .733 & \textbf{.321} & .778 \\
    Trans-FR      & FR & .000 & .000 & .000 & .928 \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.32\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{cm_multi_en.pdf}
    \caption{EN test.}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.32\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{cm_multi_de.pdf}
    \caption{DE test.}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.32\columnwidth}
    \centering
    \includegraphics[width=\linewidth]{cm_multi_fr.pdf}
    \caption{FR test (zero-shot).}
  \end{subfigure}
  \caption{Confusion matrices for the multilingual EN+DE model across languages. EN is precise (14~FP); DE/FR produce more FPs due to long medical forum posts.}
  \label{fig:cm_multi}
\end{figure}

\subsection{Discussion}

\paragraph{When multilingual training helps.}
The multilingual EN+DE model (F1\,=\,0.707) now \emph{substantially outperforms} the monolingual EN model (F1\,=\,0.621) on English---a complete reversal from early experiments where only 2~epochs were used and multilingual training hurt EN performance.
With sufficient training, the model separates language-specific and shared features; the jointly trained model sees a wider variety of negative examples, reducing EN false positives (14~FP vs.\ 19 for monolingual) while also improving recall (41~TP vs.\ 36).
For German, the monolingual model (83~positives) still collapses to all-negative predictions, but multilingual training rescues DE dramatically (F1\,=\,0.484, recall\,=\,0.629, detecting 22/35~positives) by transferring ADE patterns from the larger EN pool.

\paragraph{Zero-shot cross-lingual transfer.}
The multilingual model achieves F1\,=\,0.321 on French without any FR training data (recall\,=\,0.733, 22/30~positives detected), while the monolingual EN model achieves only F1\,=\,0.164 (10/30~detected).
Nearly all examples caught by mono-EN are also caught by multi EN+DE, which uniquely catches 13~additional positives---confirming that multilingual training provides strictly broader cross-lingual transfer.
Interestingly, the model's \textbf{recall transfers across languages better than its precision}: XLM-R captures universal ADE indicators (drug--symptom co-occurrence, complaint phrasing), but cannot learn language-specific negative-class patterns for unseen languages.
This yields a clear precision--recall gradient: EN~(0.745/0.672), DE~(0.393/0.629), FR~(0.206/0.733)---precision degrades as linguistic distance from training data increases, while recall remains strong (Figure~\ref{fig:cm_multi}).

\paragraph{Translation-trained FR: two failure modes.}
The translation-trained model demonstrates extreme sensitivity to label distribution.
Under 50/50~balanced sampling (3k~translations, baseline experiment), it achieved recall\,=\,0.90 but precision\,=\,0.125 (189~FPs), having learned ``translationese'' patterns as spurious ADE indicators.
Under natural ${\sim}7$\% positive rate (6k~translations), it predicts all-negative (F1\,=\,0.00): the ${\sim}420$~positives among 6{,}000 MT-generated examples contain too much translation noise for the model to learn any discriminative signal.
Neither extreme of label balancing produces a usable classifier, highlighting that \textbf{MT quality and naturalness}---not just sample size or class ratio---are the fundamental bottleneck.
The zero-shot multilingual model (F1\,=\,0.321) now \emph{surpasses} both translation-trained configurations, demonstrating that cross-lingual transfer from quality multilingual training outperforms naive MT-based training on this task.

\paragraph{Error analysis.}
False positives concentrate in long forum posts: DE/FR~FPs have median word counts of 176/146 words, far exceeding correctly classified negatives (71/62), suggesting the model learns the spurious heuristic \emph{long medical text\,$\Rightarrow$\,ADE}.
The \texttt{max\_length=256} truncation aggravates this by forcing decisions on partial, keyword-rich text.
English FNs are typically short, terse tweets where ADE mentions are implicit or use colloquial abbreviations (e.g., ``\textit{side fx}'' for side effects, misspelled medical terms).
French FNs have very low positive probability ($p \leq 0.003$), indicating the model is \emph{confidently wrong}---these likely represent ADEs described with French-specific vocabulary lacking equivalents in the EN/DE training data.

\paragraph{Limitations and recommendations.}
For practical deployment, F1\,=\,0.321 and precision\,=\,20.6\% on FR are insufficient for pharmacovigilance (${\sim}$4 of 5~flagged posts are false alarms).
Improvements would require: (i)~class-imbalance handling (weighted loss, focal loss, threshold tuning), (ii)~higher-quality translations or human annotation with moderate oversampling, (iii)~domain-adaptive pre-training on medical social media text, and (iv)~adding more training languages to the multilingual pool to broaden cross-lingual coverage.

%% ============================================================
\section{Task~2: Classical ML vs.\ LLMs on Structured Data}

\subsection{Data and Setup}

We use the canonical Iris dataset~\cite{fisher1936iris} (150~samples, 4~numeric features, 3~classes) with stratified train/val/test splits (96/24/30).
Three modeling paradigms are compared:
\begin{enumerate}
  \item \textbf{Classical}: Random Forest (400~trees) and Decision Tree (depth~3).
  \item \textbf{Transformer}: BERT-family models (\texttt{bert-tiny}, \texttt{-mini}, \texttt{-small}) fine-tuned on two textified representations (natural-language and compact key-value), each in cm and mm variants.
  \item \textbf{LLM few-shot}: FLAN-T5-large (780M), Llama-3.2-1B-Instruct, and Qwen2.5-1.5B-Instruct prompted with 3~balanced examples, with and without Decision Tree rules as guidance.
\end{enumerate}

\paragraph{Textification.}
Each row is converted to two text formats (e.g., \emph{natural}: ``An iris flower has sepal length 51\,mm \ldots'' and \emph{compact}: ``sepal\_length=51\,mm; \ldots'').
Features are converted from cm to integer mm to reduce subword tokenization fragmentation (``5.1'' $\to$ three tokens vs.\ ``51'' $\to$ one).

\subsection{Results}

\paragraph{Classical baselines.}
Both RF and DT achieve macro-F1\,=\,0.933, with only versicolor$\leftrightarrow$virginica confusion.
RF feature importances confirm petal width (0.465) and petal length (0.398) as dominant discriminators; sepal width contributes only 0.014.
The DT captures the decision surface in 3~rules: \emph{petal\_length $\leq$24\,mm $\Rightarrow$ setosa; petal\_width $\leq$16\,mm $\Rightarrow$ versicolor; else virginica}.

\paragraph{Transformers on textified data.}
Table~\ref{tab:task2_transf} shows the best model per textification.
\texttt{bert-mini} and \texttt{bert-small} reach 0.933 macro-F1 on three of four representations, matching RF. \texttt{bert-tiny} collapses to near-chance (omitted).
The mm encoding consistently outperforms cm, confirming that subword fragmentation of decimals degrades learning.

\begin{table}[t]
  \caption{Task~2 Transformer results: best model per textification on the Iris test set (30~samples).}
  \label{tab:task2_transf}
  \begin{tabular}{llrrr}
    \toprule
    Repr. & Best model & Acc. & F1 & Epoch \\
    \midrule
    compact\_cm & bert-small & .933 & .933 & 10 \\
    compact\_mm & bert-mini & .933 & .933 & 20 \\
    natural\_cm & bert-small & .900 & .900 & 21 \\
    natural\_mm & bert-small & .933 & .933 & 9 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{LLM few-shot.}
Table~\ref{tab:task2_llm} shows all LLM results.
FLAN-T5-large predicts ``setosa'' for every sample (F1\,=\,0.167), both unguided and guided. It produces no reasoning---only the bare token---ignoring the prompt structure entirely.
This is plausibly due to (i)~the lack of a chat template (the prompt is raw text to the encoder), (ii)~the seq2seq architecture defaulting to the first plausible label, and (iii)~the absence of numeric reasoning capacity at this scale.

Llama and Qwen benefit critically from \textbf{chat templates}, which frame the prompt with role markers. Without this, instruct-tuned decoder models would likely degenerate similarly to T5. Guidance improves Llama by +0.067 F1 and Qwen by +0.007 F1; the larger effect on Llama suggests weaker models benefit more from explicit rules.

\begin{table}[t]
  \caption{Task~2 LLM few-shot results (Iris test, 30 samples). Cov.: parseable outputs.}
  \label{tab:task2_llm}
  \begin{tabular}{llrrrr}
    \toprule
    Model & Mode & Acc. & F1 & Cov. \\
    \midrule
    FLAN-T5-large & unguided & .333 & .167 & 30 \\
    FLAN-T5-large & guided & .333 & .167 & 30 \\
    Llama-3.2-1B & unguided & .600 & .489 & 30 \\
    Llama-3.2-1B & guided & .667 & .556 & 30 \\
    Qwen2.5-1.5B & unguided & .759 & .750 & 29 \\
    Qwen2.5-1.5B & guided & \textbf{.767} & \textbf{.757} & 30 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Reasoning quality and errors.}
Llama and Qwen produce structured reasoning; FLAN-T5 produces none. Inspection reveals that misclassifications stem from faulty numeric reasoning near the versicolor/virginica boundary. For example, Llama unguided outputs ``\emph{Petal length is large but petal width is small}'' for a sample with petal\_width=18\,mm (true: virginica), misclassifying it as versicolor because vague qualitative reasoning cannot distinguish boundary cases. Even guided, Llama sometimes misapplies rules---e.g., stating ``\emph{petal\_width (18\,mm) $\leq$ 16\,mm}'' to justify versicolor, an arithmetically incorrect comparison.
Qwen applies thresholds more reliably (e.g., ``\emph{petal\_length=47\,mm > 24\,mm, petal\_width=12\,mm $\leq$ 16\,mm $\Rightarrow$ versicolor}'') but still errs near boundaries. Notably, some correct predictions come with wrong reasoning, confirming that LLM explanations are post-hoc rationalizations, not faithful decision traces.

\subsection{Discussion}

Table~\ref{tab:task2_summary} (Appendix) gives the full comparison across all 24~runs.

\paragraph{Inductive bias and sample efficiency.}
RF achieves 0.933~F1 from 96~samples in milliseconds. Transformers match this only with sufficient capacity (bert-mini/small) and careful textification, requiring ${\sim}$10--20 epochs of GPU fine-tuning. LLMs use no training data but encode billions of pretraining tokens---this general knowledge does not transfer to precise numeric thresholds.

\paragraph{Robustness.}
LLMs are the least robust approach: (i)~prompt format dramatically affects results (FLAN-T5's collapse vs.\ Qwen's 0.757 on the same task), (ii)~output parsing is fragile (Qwen produced 1~unparseable output in unguided mode, fixed by guidance), and (iii)~three models of comparable size (780M--1.5B) yield F1 from 0.167 to 0.757. Transformers are sensitive to representation (cm vs mm) and model size (bert-tiny collapses while bert-small succeeds).

\paragraph{Interpretability.}
DT rules are the gold standard. RF importances are informative but incomplete. Transformer decisions require dedicated attribution methods. LLM reasoning traces appear interpretable but are unreliable---correct predictions sometimes accompany incorrect reasoning.

\paragraph{Practical recommendations.}
Classical models are optimal for small structured data. Transformers are viable when a text interface is needed. LLM prompting is useful for rapid prototyping and explanation generation but should not be trusted for numeric classification without hybrid augmentation, and even then the gains are modest (Qwen guided: 0.757 vs RF: 0.933).

%% ============================================================
\section{Conclusion}

Task~1 demonstrates that multilingual training with XLM-RoBERTa, given sufficient epochs, is the single most effective strategy for cross-lingual ADE detection: it achieves the best F1 on all three languages simultaneously (EN\,0.707, DE\,0.484, FR zero-shot\,0.321), reversing early findings that multilingual training hurts dominant-language performance.
Translation-based training proves unreliable---collapsing to all-negative or all-positive predictions depending on label distribution---confirming that MT quality, not merely data quantity, is the bottleneck for cross-lingual ADE systems.
Precision on unseen languages remains the key limitation, driven by an asymmetry in what transfers cross-lingually: recall generalises better than precision.
Task~2 confirms that classical ML remains unbeatable on small structured datasets, that Transformers can learn from textified tabular data given sufficient model capacity, and that current LLMs fundamentally struggle with precise numeric reasoning in few-shot settings.
Together, these findings underscore the importance of matching model inductive biases to data characteristics rather than defaulting to the largest available model.

%% ============================================================
% \bibliographystyle{ACM-Reference-Format}
% \bibliography{references}

%% ============================================================
\appendix

\section{Text Samples}
\label{app:samples}

\subsection{Task~1: ADE Examples Across Languages}

\paragraph{English (positive ADE).}
``\texttt{@USER I've been on metformin for 2 months and the nausea is unbearable. Can barely eat anything.}''

\paragraph{German (positive ADE).}
``\texttt{Seit ich Ibuprofen nehme, habe ich st\"andig Magenprobleme und Schwindel. Der Arzt hat die Dosis erh\"oht und es wurde schlimmer.}''
(Translation: Since taking ibuprofen, I have constant stomach problems and dizziness.)

\paragraph{French---native (negative).}
``\texttt{Mon m\'edecin m'a prescrit du parac\'etamol pour la fi\`evre. Je le prends depuis trois jours sans probl\`eme.}''
(Translation: My doctor prescribed paracetamol for fever. I have been taking it for three days without problems.)

\paragraph{French---machine-translated (positive, from EN).}
Original EN: ``\texttt{@USER this lexapro making me so tired I can't function}'' \\
MT FR: ``\texttt{@USER ce lexapro me rend si fatigu\'e que je ne peux pas fonctionner}''

\noindent Note: hashtags and informal tokens (e.g., \texttt{@USER}) are preserved verbatim by MarianMT, and ``translationese'' is evident in the overly literal structure.

\subsection{Task~2: Textification Examples}

\paragraph{Natural language (mm).}
``An iris flower has sepal length 51\,mm, sepal width 35\,mm, petal length 14\,mm, and petal width 2\,mm.'' $\to$ setosa

\paragraph{Compact format (mm).}
``sepal\_length=51\,mm; sepal\_width=35\,mm; petal\_length=14\,mm; petal\_width=2\,mm'' $\to$ setosa

\paragraph{Decision tree guidance (injected into LLM prompt).}
``If petal\_length $\leq$ 24\,mm, it is setosa. Else if petal\_width $\leq$ 16\,mm, it is versicolor. Otherwise, it is virginica.''

\section{Task~2: Full Model Comparison}
\label{app:task2_full}

Table~\ref{tab:task2_summary} reports all 24~experiment runs for Task~2, spanning classical, Transformer, and LLM approaches.

\begin{table*}[t]
  \caption{Task~2 comprehensive results on the Iris test set (30~samples). Summary statistics by approach are shown below the main table.}
  \label{tab:task2_summary}
  \small
  \begin{tabular}{lllllrr}
    \toprule
    Approach & Model & Architecture & Learning & Repr. & F1 & Acc. \\
    \midrule
    Classical & RandomForest & Ensemble Trees & Training & structured & \textbf{.933} & .933 \\
    Classical & DT (depth 3) & Single Tree & Training & structured & .933 & .933 \\
    \midrule
    Transformer & bert-small & Encoder & Fine-tune & compact\_cm & .933 & .933 \\
    Transformer & bert-mini & Encoder & Fine-tune & compact\_mm & .933 & .933 \\
    Transformer & bert-small & Encoder & Fine-tune & natural\_mm & .933 & .933 \\
    Transformer & bert-small & Encoder & Fine-tune & natural\_cm & .900 & .900 \\
    Transformer & bert-tiny & Encoder & Fine-tune & (various) & .333 & .333 \\
    \midrule
    LLM few-shot & Qwen2.5-1.5B & Decoder & ICL & compact\_mm & .750 & .759 \\
    LLM + guidance & Qwen2.5-1.5B & Decoder & ICL & compact\_mm & \textbf{.757} & .767 \\
    LLM few-shot & Llama-3.2-1B & Decoder & ICL & compact\_mm & .489 & .600 \\
    LLM + guidance & Llama-3.2-1B & Decoder & ICL & compact\_mm & .556 & .667 \\
    LLM few-shot & FLAN-T5-large & Enc-Dec & ICL & compact\_mm & .167 & .333 \\
    LLM + guidance & FLAN-T5-large & Enc-Dec & ICL & compact\_mm & .167 & .333 \\
    \midrule
    \multicolumn{5}{l}{\textbf{Summary by approach}} & \textbf{mean / max F1} & \textbf{mean / max Acc.} \\
    \midrule
    \multicolumn{5}{l}{Classical} & .933 / .933 & .933 / .933 \\
    \multicolumn{5}{l}{Transformer} & .658 / .933 & .708 / .933 \\
    \multicolumn{5}{l}{LLM Few-shot} & .468 / .750 & .564 / .759 \\
    \multicolumn{5}{l}{LLM + Classical Guidance} & .493 / .757 & .589 / .767 \\
    \bottomrule
  \end{tabular}
\end{table*}

\end{document}
