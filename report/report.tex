\documentclass[sigconf]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{Bib\TeX}}}

\setcopyright{none}
\acmYear{2026}
\acmDOI{}
\acmConference[Advanced NLP 2026]{Advanced NLP Exercise Report}{February 2026}{TU Berlin, Germany}
\acmISBN{}

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}

\begin{document}

\title{Multilingual ADE Detection and Classical ML vs.\ LLMs on Structured Data}

\author{Andrés Mandado Almajano}
\affiliation{%
  \institution{TU Berlin}
  \country{Germany}}
\email{mandadoalmajano@tu-berlin.de}

\renewcommand{\shortauthors}{Andrés Mandado Almajano}

\begin{abstract}
We present two NLP studies.
\textbf{Task~1} addresses multilingual detection of adverse drug events (ADEs) in social media using XLM-RoBERTa, comparing monolingual, multilingual, and machine-translation--based training across English, German, and French.
Results show monolingual English training yields the best English F1 (0.585), while multilingual training provides marginal cross-lingual transfer but degrades the dominant language.
Translation-trained French models achieve higher recall (0.90) than zero-shot approaches but suffer from very low precision (0.125).
\textbf{Task~2} compares classical tree-based models, fine-tuned Transformers on textified data, and few-shot LLM prompting (FLAN-T5, Llama-3.2, Qwen2.5) on the Iris dataset.
The Random Forest and best Transformers both achieve 93.3\% macro-F1, while LLMs range from 16.7\% (FLAN-T5, single-class collapse) to 75.7\% (Qwen guided), demonstrating the fundamental mismatch between LLMs and small tabular tasks and the unreliability of LLM reasoning on numeric data.
\end{abstract}

\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010147.10010178.10010179</concept_id>
  <concept_desc>Computing methodologies~Natural language processing</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010147.10010257</concept_id>
  <concept_desc>Computing methodologies~Machine learning</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language processing}
\ccsdesc[300]{Computing methodologies~Machine learning}

\keywords{adverse drug events, multilingual NLP, cross-lingual transfer, machine translation, structured data, LLMs, textification}

\maketitle

%% ============================================================
\section{Introduction}

This report presents two complementary studies in applied NLP.
\textbf{Task~1} tackles SMM4H~2026 Shared Task~1~\cite{weissenbacher2022smm4h}: binary detection of adverse drug events (ADEs) in multilingual social media posts (English tweets, German and French forum posts).
We study (i)~monolingual vs.\ multilingual training with XLM-RoBERTa~\cite{conneau2020xlmr}, (ii)~zero-shot cross-lingual transfer to unseen French, and (iii)~machine-translation--based training using MarianMT~\cite{junczys2018marianmt} EN$\to$FR translations.
\textbf{Task~2} compares classical ML (Random Forest~\cite{breiman2001rf}), fine-tuned Transformer classifiers~\cite{devlin2019bert}, and few-shot LLM prompting~\cite{chung2022flant5} on the Iris dataset~\cite{fisher1936iris}, investigating when modern NLP tools surpass or fall short of simple baselines on small structured data.

All experiments use the Hugging Face \texttt{transformers} library~\cite{wolf2020transformers} with fixed random seeds for reproducibility.

%% ============================================================
\section{Task~1: Multilingual ADE Detection}

\subsection{Data and Preprocessing}

The SMM4H~2026 Task~1 dataset contains social media posts labeled for ADE presence (binary).
We use English (EN, 17{,}128 train / 888 test), German (DE, 1{,}482 train / 634 test), and French (FR, 976 train / 418 test).
The provided development set is redefined as the \textbf{test set}; a new \textbf{validation set} (10\%) is created from the training data via stratified sampling by language$\times$label.
French data is held out entirely for zero-shot evaluation.

Table~\ref{tab:task1_data} summarizes key dataset statistics.
All languages exhibit severe class imbalance (${\sim}5$--$7$\% positive rate).
English data consists of short tweets ($\bar{w}\approx17$ words), while German and French are long forum posts ($\bar{w}\approx101$--$106$ words), creating a substantial domain mismatch.

\begin{table}[t]
  \caption{Task~1 dataset statistics (train split). $\bar{w}$: mean word count; $w_{95}$: 95th-percentile word count.}
  \label{tab:task1_data}
  \begin{tabular}{lrrrr}
    \toprule
    Lang & $n$ & Pos.\ rate & $\bar{w}$ & $w_{95}$ \\
    \midrule
    EN & 17{,}128 & 7.0\% & 16.5 & 27 \\
    DE & 1{,}482 & 5.6\% & 100.8 & 276 \\
    FR & 976 & 7.2\% & 105.8 & 313 \\
    \bottomrule
  \end{tabular}
\end{table}

Preprocessing is minimal and ADE-preserving: \texttt{@USER\_\_\_} patterns are collapsed to \texttt{@USER}, \texttt{HTTPURL\_\_\_} to \texttt{HTTPURL}, and the forum placeholder \texttt{<user>} is normalized.
No lowercasing is applied to preserve multilingual casing.

\paragraph{Machine translation.}
We translate 3{,}000 English training examples (stratified by label) to French using the offline MarianMT model (\texttt{Helsinki-NLP/opus-mt-en-fr}).
Translations are cached to disk; manual inspection reveals that hashtags and informal expressions are often transferred literally rather than translated (see Appendix~\ref{app:samples}).

\subsection{Modeling Setups}

All classifiers use \texttt{xlm-roberta-base} (278M parameters) with a sequence classification head, max length 256, and are trained for 2~epochs with learning rate $2{\times}10^{-5}$ and batch size~16.
The same code path is used across four setups:
\begin{enumerate}
  \item \textbf{Mono-EN}: train on EN, evaluate on EN test.
  \item \textbf{Mono-DE}: train on DE, evaluate on DE test.
  \item \textbf{Multi EN+DE}: train on EN+DE jointly, evaluate on EN/DE/FR test.
  \item \textbf{Trans-FR}: train on EN$\to$FR translations, evaluate on FR test.
\end{enumerate}

\subsection{Results}

Table~\ref{tab:task1_results} reports binary F1, precision, recall, and accuracy for all setups.

\begin{table}[t]
  \caption{Task~1 evaluation results on the test set. Best F1 per language in \textbf{bold}.}
  \label{tab:task1_results}
  \begin{tabular}{llrrrr}
    \toprule
    Setup & Lang & Prec. & Rec. & F1 & Acc. \\
    \midrule
    Mono-EN       & EN & .581 & .590 & \textbf{.585} & .943 \\
    Multi EN+DE   & EN & .574 & .508 & .539 & .940 \\
    \midrule
    Mono-DE       & DE & .000 & .000 & .000 & .945 \\
    Multi EN+DE   & DE & 1.00 & .029 & \textbf{.056} & .946 \\
    \midrule
    Mono-EN (zs)  & FR & .000 & .000 & .000 & .907 \\
    Mono-DE (zs)  & FR & .000 & .000 & .000 & .928 \\
    Multi EN+DE   & FR & .097 & .100 & .098 & .868 \\
    Trans-FR      & FR & .125 & .900 & \textbf{.220} & .541 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Discussion}

\paragraph{Monolingual vs.\ multilingual (EN).}
The monolingual EN model (F1\,=\,0.585) outperforms the multilingual model on English (F1\,=\,0.539).
Adding German forum posts \emph{hurts} English tweet classification: the domain gap (short tweets vs.\ long medical forums) outweighs any benefit from additional data.
Recall drops by 8~pp, confirming that the model partially reweights its features toward German-specific patterns.

\paragraph{German.}
The monolingual DE model (1{,}334 train, ${\sim}83$ positives) achieves zero recall---the extreme imbalance and tiny training set make it impossible to learn the positive class.
Multilingual training marginally helps (F1\,=\,0.056), transferring some ADE signal from the larger English pool.

\paragraph{Zero-shot French.}
Both monolingual models produce F1\,=\,0.00 on French (all-negative predictions).
The multilingual EN+DE model achieves F1\,=\,0.098 (3/30 positives detected), showing minimal cross-lingual transfer.
The domain gap (tweets vs.\ forums), combined with the absence of French training signal, makes pure zero-shot transfer insufficient.

\paragraph{Translation-trained French.}
Training on 3{,}000 EN$\to$FR translations yields F1\,=\,0.220 with recall\,=\,0.90 but precision\,=\,0.125.
The model detects 27/30 ADEs but generates 189 false positives (FP) out of 388 negatives.
This over-prediction is likely driven by ``translationese'' artifacts---hallucinated ADE-like patterns in the generated French text that the model learns as spurious features.
For pharmacovigilance, a precision of 12.5\% is impractical: 7~of 8 flagged posts would be false alarms.
Nonetheless, the translated training approach is strictly necessary to achieve \emph{any} recall on French.

\paragraph{Takeaway.}
For practical multilingual ADE systems, neither zero-shot transfer nor naive MT-based training alone suffices. Improvements would require: (i)~class-imbalance mitigation (weighted loss, focal loss), (ii)~higher-quality translations or human annotation, and (iii)~continued pre-training on in-domain medical social media text.

%% ============================================================
\section{Task~2: Classical ML vs.\ LLMs on Structured Data}

\subsection{Data and Setup}

We use the canonical Iris dataset~\cite{fisher1936iris} (150~samples, 4~numeric features, 3~classes) with stratified train/val/test splits (96/24/30).
Three modeling paradigms are compared:
\begin{enumerate}
  \item \textbf{Classical}: Random Forest (400~trees) and Decision Tree (depth~3).
  \item \textbf{Transformer}: BERT-family models (\texttt{bert-tiny}, \texttt{-mini}, \texttt{-small}) fine-tuned on two textified representations (natural-language and compact key-value), each in cm and mm variants.
  \item \textbf{LLM few-shot}: FLAN-T5-large (780M), Llama-3.2-1B-Instruct, and Qwen2.5-1.5B-Instruct prompted with 3~balanced examples, with and without Decision Tree rules as guidance.
\end{enumerate}

\paragraph{Textification.}
Each row is converted to two text formats (e.g., \emph{natural}: ``An iris flower has sepal length 51\,mm \ldots'' and \emph{compact}: ``sepal\_length=51\,mm; \ldots'').
Features are converted from cm to integer mm to reduce subword tokenization fragmentation (``5.1'' $\to$ three tokens vs.\ ``51'' $\to$ one).

\subsection{Results}

\paragraph{Classical baselines.}
Both RF and DT achieve macro-F1\,=\,0.933, with only versicolor$\leftrightarrow$virginica confusion.
RF feature importances confirm petal width (0.465) and petal length (0.398) as dominant discriminators; sepal width contributes only 0.014.
The DT captures the decision surface in 3~rules: \emph{petal\_length $\leq$24\,mm $\Rightarrow$ setosa; petal\_width $\leq$16\,mm $\Rightarrow$ versicolor; else virginica}.

\paragraph{Transformers on textified data.}
Table~\ref{tab:task2_transf} shows the best model per textification.
\texttt{bert-mini} and \texttt{bert-small} reach 0.933 macro-F1 on three of four representations, matching RF. \texttt{bert-tiny} collapses to near-chance (omitted).
The mm encoding consistently outperforms cm, confirming that subword fragmentation of decimals degrades learning.

\begin{table}[t]
  \caption{Task~2 Transformer results: best model per textification on the Iris test set (30~samples).}
  \label{tab:task2_transf}
  \begin{tabular}{llrrr}
    \toprule
    Repr. & Best model & Acc. & F1 & Epoch \\
    \midrule
    compact\_cm & bert-small & .933 & .933 & 10 \\
    compact\_mm & bert-mini & .933 & .933 & 20 \\
    natural\_cm & bert-small & .900 & .900 & 21 \\
    natural\_mm & bert-small & .933 & .933 & 9 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{LLM few-shot.}
Table~\ref{tab:task2_llm} shows all LLM results.
FLAN-T5-large predicts ``setosa'' for every sample (F1\,=\,0.167), both unguided and guided. It produces no reasoning---only the bare token---ignoring the prompt structure entirely.
This is plausibly due to (i)~the lack of a chat template (the prompt is raw text to the encoder), (ii)~the seq2seq architecture defaulting to the first plausible label, and (iii)~the absence of numeric reasoning capacity at this scale.

Llama and Qwen benefit critically from \textbf{chat templates}, which frame the prompt with role markers. Without this, instruct-tuned decoder models would likely degenerate similarly to T5. Guidance improves Llama by +0.067 F1 and Qwen by +0.007 F1; the larger effect on Llama suggests weaker models benefit more from explicit rules.

\begin{table}[t]
  \caption{Task~2 LLM few-shot results (Iris test, 30 samples). Cov.: parseable outputs.}
  \label{tab:task2_llm}
  \begin{tabular}{llrrrr}
    \toprule
    Model & Mode & Acc. & F1 & Cov. \\
    \midrule
    FLAN-T5-large & unguided & .333 & .167 & 30 \\
    FLAN-T5-large & guided & .333 & .167 & 30 \\
    Llama-3.2-1B & unguided & .600 & .489 & 30 \\
    Llama-3.2-1B & guided & .667 & .556 & 30 \\
    Qwen2.5-1.5B & unguided & .759 & .750 & 29 \\
    Qwen2.5-1.5B & guided & \textbf{.767} & \textbf{.757} & 30 \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Reasoning quality and errors.}
Llama and Qwen produce structured reasoning; FLAN-T5 produces none. Inspection reveals that misclassifications stem from faulty numeric reasoning near the versicolor/virginica boundary. For example, Llama unguided outputs ``\emph{Petal length is large but petal width is small}'' for a sample with petal\_width=18\,mm (true: virginica), misclassifying it as versicolor because vague qualitative reasoning cannot distinguish boundary cases. Even guided, Llama sometimes misapplies rules---e.g., stating ``\emph{petal\_width (18\,mm) $\leq$ 16\,mm}'' to justify versicolor, an arithmetically incorrect comparison.
Qwen applies thresholds more reliably (e.g., ``\emph{petal\_length=47\,mm > 24\,mm, petal\_width=12\,mm $\leq$ 16\,mm $\Rightarrow$ versicolor}'') but still errs near boundaries. Notably, some correct predictions come with wrong reasoning, confirming that LLM explanations are post-hoc rationalizations, not faithful decision traces.

\subsection{Discussion}

Table~\ref{tab:task2_summary} (Appendix) gives the full comparison across all 24~runs.

\paragraph{Inductive bias and sample efficiency.}
RF achieves 0.933~F1 from 96~samples in milliseconds. Transformers match this only with sufficient capacity (bert-mini/small) and careful textification, requiring ${\sim}$10--20 epochs of GPU fine-tuning. LLMs use no training data but encode billions of pretraining tokens---this general knowledge does not transfer to precise numeric thresholds.

\paragraph{Robustness.}
LLMs are the least robust approach: (i)~prompt format dramatically affects results (FLAN-T5's collapse vs.\ Qwen's 0.757 on the same task), (ii)~output parsing is fragile (Qwen produced 1~unparseable output in unguided mode, fixed by guidance), and (iii)~three models of comparable size (780M--1.5B) yield F1 from 0.167 to 0.757. Transformers are sensitive to representation (cm vs mm) and model size (bert-tiny collapses while bert-small succeeds).

\paragraph{Interpretability.}
DT rules are the gold standard. RF importances are informative but incomplete. Transformer decisions require dedicated attribution methods. LLM reasoning traces appear interpretable but are unreliable---correct predictions sometimes accompany incorrect reasoning.

\paragraph{Practical recommendations.}
Classical models are optimal for small structured data. Transformers are viable when a text interface is needed. LLM prompting is useful for rapid prototyping and explanation generation but should not be trusted for numeric classification without hybrid augmentation, and even then the gains are modest (Qwen guided: 0.757 vs RF: 0.933).

%% ============================================================
\section{Conclusion}

Task~1 demonstrates that cross-lingual ADE detection in social media faces compounding challenges: domain mismatch (tweets vs.\ forums), severe class imbalance, and translation artifacts.
Monolingual training is best for the dominant language, but some form of translated or in-language data is essential for unseen languages---though naive MT introduces unacceptable false-positive rates.
Task~2 confirms that classical ML remains unbeatable on small structured datasets, that Transformers can learn from textified tabular data given sufficient model capacity, and that current LLMs fundamentally struggle with precise numeric reasoning in few-shot settings.
Together, these findings underscore the importance of matching model inductive biases to data characteristics rather than defaulting to the largest available model.

%% ============================================================
% \bibliographystyle{ACM-Reference-Format}
% \bibliography{references}

%% ============================================================
\appendix

\section{Text Samples}
\label{app:samples}

\subsection{Task~1: ADE Examples Across Languages}

\paragraph{English (positive ADE).}
``\texttt{@USER I've been on metformin for 2 months and the nausea is unbearable. Can barely eat anything.}''

\paragraph{German (positive ADE).}
``\texttt{Seit ich Ibuprofen nehme, habe ich st\"andig Magenprobleme und Schwindel. Der Arzt hat die Dosis erh\"oht und es wurde schlimmer.}''
(Translation: Since taking ibuprofen, I have constant stomach problems and dizziness.)

\paragraph{French---native (negative).}
``\texttt{Mon m\'edecin m'a prescrit du parac\'etamol pour la fi\`evre. Je le prends depuis trois jours sans probl\`eme.}''
(Translation: My doctor prescribed paracetamol for fever. I have been taking it for three days without problems.)

\paragraph{French---machine-translated (positive, from EN).}
Original EN: ``\texttt{@USER this lexapro making me so tired I can't function}'' \\
MT FR: ``\texttt{@USER ce lexapro me rend si fatigu\'e que je ne peux pas fonctionner}''

\noindent Note: hashtags and informal tokens (e.g., \texttt{@USER}) are preserved verbatim by MarianMT, and ``translationese'' is evident in the overly literal structure.

\subsection{Task~2: Textification Examples}

\paragraph{Natural language (mm).}
``An iris flower has sepal length 51\,mm, sepal width 35\,mm, petal length 14\,mm, and petal width 2\,mm.'' $\to$ setosa

\paragraph{Compact format (mm).}
``sepal\_length=51\,mm; sepal\_width=35\,mm; petal\_length=14\,mm; petal\_width=2\,mm'' $\to$ setosa

\paragraph{Decision tree guidance (injected into LLM prompt).}
``If petal\_length $\leq$ 24\,mm, it is setosa. Else if petal\_width $\leq$ 16\,mm, it is versicolor. Otherwise, it is virginica.''

\section{Task~2: Full Model Comparison}
\label{app:task2_full}

Table~\ref{tab:task2_summary} reports all 24~experiment runs for Task~2, spanning classical, Transformer, and LLM approaches.

\begin{table*}[t]
  \caption{Task~2 comprehensive results on the Iris test set (30~samples). Summary statistics by approach are shown below the main table.}
  \label{tab:task2_summary}
  \small
  \begin{tabular}{lllllrr}
    \toprule
    Approach & Model & Architecture & Learning & Repr. & F1 & Acc. \\
    \midrule
    Classical & RandomForest & Ensemble Trees & Training & structured & \textbf{.933} & .933 \\
    Classical & DT (depth 3) & Single Tree & Training & structured & .933 & .933 \\
    \midrule
    Transformer & bert-small & Encoder & Fine-tune & compact\_cm & .933 & .933 \\
    Transformer & bert-mini & Encoder & Fine-tune & compact\_mm & .933 & .933 \\
    Transformer & bert-small & Encoder & Fine-tune & natural\_mm & .933 & .933 \\
    Transformer & bert-small & Encoder & Fine-tune & natural\_cm & .900 & .900 \\
    Transformer & bert-tiny & Encoder & Fine-tune & (various) & .333 & .333 \\
    \midrule
    LLM few-shot & Qwen2.5-1.5B & Decoder & ICL & compact\_mm & .750 & .759 \\
    LLM + guidance & Qwen2.5-1.5B & Decoder & ICL & compact\_mm & \textbf{.757} & .767 \\
    LLM few-shot & Llama-3.2-1B & Decoder & ICL & compact\_mm & .489 & .600 \\
    LLM + guidance & Llama-3.2-1B & Decoder & ICL & compact\_mm & .556 & .667 \\
    LLM few-shot & FLAN-T5-large & Enc-Dec & ICL & compact\_mm & .167 & .333 \\
    LLM + guidance & FLAN-T5-large & Enc-Dec & ICL & compact\_mm & .167 & .333 \\
    \midrule
    \multicolumn{5}{l}{\textbf{Summary by approach}} & \textbf{mean / max F1} & \textbf{mean / max Acc.} \\
    \midrule
    \multicolumn{5}{l}{Classical} & .933 / .933 & .933 / .933 \\
    \multicolumn{5}{l}{Transformer} & .658 / .933 & .708 / .933 \\
    \multicolumn{5}{l}{LLM Few-shot} & .468 / .750 & .564 / .759 \\
    \multicolumn{5}{l}{LLM + Classical Guidance} & .493 / .757 & .589 / .767 \\
    \bottomrule
  \end{tabular}
\end{table*}

\end{document}
